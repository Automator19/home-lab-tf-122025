# ðŸš€ Component: Application Layer (EC2) & Project Structure

## 1. Overview
This document covers the **Application Layer** of the Home Lab. It describes both the reusable "Blueprint" (the Terraform module) and the "Live Implementation" (how we organize projects like Grafana and Ansible).

**Core Philosophy:** We separate **Logic** (Terraform) from **Data** (YAML Configuration) and **State** (Terragrunt Projects) to create a scalable, crash-resistant architecture.

---

## 2. The Blueprint (Module Logic)
*Location: `modules/ec2`*

The module acts as a "Server Factory." Instead of defining static servers, it contains logic to dynamically generate infrastructure based on inputs.

### Key Architecture Decisions

#### A. Configuration-as-Data (The "Dynamic Loader")
* **What we did:** We built logic that automatically scans a specific folder for YAML files. For every file it finds, it loops through and creates one EC2 instance.
* **Why we did it:** This separates *Logic* from *Data*. To add a new server, we simply drop a new file into a folder. We never have to edit the complex Terraform code, reducing the risk of accidental syntax errors.

#### B. Auto-Naming Strategy
* **What we did:** We configured the module to use the **filename** directly as the **Server Name**.
    * *Example:* `dashboard.yaml` $\rightarrow$ Server Name: `dashboard`
* **Why we did it:** This aligns the code with reality. It makes it instantly obvious in the AWS Console which file controls which server, simplifying troubleshooting.

#### C. Cost Optimization (Spot Instances)
* **What we did:** We hardcoded the module to always request **AWS Spot Instances** (`market_type="spot"`) rather than On-Demand instances.
* **Why we did it:** For a Home Lab, cost is the priority. Spot instances offer a 70-90% discount, allowing us to run multiple servers for pennies per hour.

#### D. Security Isolation
* **What we did:** We ensured that every time this module is used, it creates a unique **Security Group** specific to that project stack.
* **Why we did it:** This follows the **Principle of Least Privilege**. The Ansible servers have their own firewall, and the Grafana servers have theirs. If one project is compromised, the others remain secure.

---

## 3. The Implementation (Live Setup)
*Location: `live/dev/us-east-1/...`*

We adopted a **Project-Based Hierarchy**. Instead of dumping all servers into one generic "ec2" folder, we created dedicated folders for each logical project.

### A. Directory Structure Strategy

**The Layout:**
* **`vpc/`**: The Network Layer (Shared).
* **`ansible/`**: Project A (Own State File).
* **`grafana/`**: Project B (Own State File).

**Why we did it:**
1.  **Reduced Blast Radius:** Running a `destroy` command in the `grafana` folder cannot physically touch the `ansible` servers. They use completely isolated State Files.
2.  **Organization:** As the lab grows to 20+ servers, grouping them by project prevents file clutter and confusion.

### B. The Deployment Workflow
We established a file-based workflow for scaling infrastructure that mimics GitOps practices.

| Action | Workflow |
| :--- | :--- |
| **Add a Server** | Create a new YAML file (e.g., `web.yaml`) in the project's config folder. |
| **Remove a Server** | Delete the YAML file. |
| **Deploy** | Run `terragrunt apply`. The logic detects the file change and updates AWS. |

### C. Dependency Management
* **What we did:** We used Terragrunt's `dependency` blocks to link the Application projects to the Network (VPC) layer.
* **Why we did it:** This ensures the Network exists before we try to launch servers into it. It also allows us to run `terragrunt run-all destroy`, knowing the tool will automatically kill the Apps *before* deleting the Network, preventing "Dependency Violation" errors.